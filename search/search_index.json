{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Passing the Torch","text":"<p>Where I teach myself PyTorch, and explore new capabilities. You are most welcome to learn along, or also to suggest corrections/additions.</p>"},{"location":"autograd/","title":"Autograd support with PyTorch tensors","text":"<p>The training for neural network often involves gradient descent. In gradient decent, which is an optimization method to find as good as possible values for parameters, such that the function calculated by the network shall give \"best\" objective function. The idea is that we look at the current output of the network and compare it to the desired output (to the target). If we call the difference (or some related function) the error. Then we want to minimize the error. We try to reduce the error a bit at any iteration. This process involves calculating gradients (partial derivaties). For example, let's say this is our function:</p> <p>\\hat{y} = a * x + 5</p> <p>Where a is the parameter, and at the start a = 1. We get an example of x = 1, and y = 7. It is not there yet, as for us we get 1 * 1 + 5 = 6. We note that \\frac{\\partial a}{\\partial y} = 1 In other words, a tiny increase of a say by \\epsilon a, shall result in a matching increase in the output. Let's increase a to 1.01. Now we get 1.01 * 1 + 5 = 6.01 which is indeed closer to the target. We actually work usually on the error, for example a * x + 5 - 7, calculate what will make it higher and go the other direction. The idea is the same. The derivatives are often more involved from above, as of multiple neurons, non-linearity, and as of composite functions. When a neuaral network is composed of multiple layers, this is basicly a composition of functions; L2(L1(Input)) and so. Calculating derivatives for composite functions is done usually with the help of the chain-rule, yet still involves a lot of calculations and is confusing.</p> <p>With neural networks of many parameters, we first calculate the gradients with respect to the error (or loss), for all the parameters, then change accordingly at the same time, a bit for each of the relevant parameters proportional to its gradient wrt the loss. As said above, coming up with the right formula of the partial derivates for all the parameters can be confusing and error prone. Luckily for us, with PyTorch we have a built-in support in the tensors, so that when you calculate a new tensor based on existing tensors, the existing tensors keep track of the gradients that are relevant to the calculation.</p> <pre><code># Create a tensor and indicate that we want to compute gradients\nx = torch.tensor(1.0, requires_grad=True)\n\n# Define a function of x\ny = x ** 2\n\n# Compute the gradient of y with respect to x\ny.backward()\n\n# The gradient is stored in x.grad\nprint(x.grad)  # Output: tensor(2.)\n</code></pre> <p>If we continue the example from above:</p> <pre><code>y2 = x ** 3\ny2.backward()\nprint(x.grad)\n</code></pre> <p><code>tensor(5.)</code></p> <p>It seems that the gradients for x were accumulated. Therefore we got both the gradients due to y as well as those due to y2.</p> <p>To reset and start from fresh:</p> <pre><code>y2 = x ** 3\nx.grad = None\ny2.backward()\nprint(x.grad)\n</code></pre> <p><code>tensor(3.)</code></p> <p>The calculations can be more involved, and as hinted above, involve multiple tensors and composed as in:</p> <pre><code>p1 = torch.tensor(1.0, requires_grad=True)\np2_1 = torch.tensor(2.0, requires_grad=True)\np2_2 = torch.tensor(3.0, requires_grad=True)\np3 = torch.tensor(1.5, requires_grad=True)\n\nx = 3\no1 = (p1 * x) ** 2 + 1\no2_1 = (p2_1 * o1) ** 0.5 - 4 \no2_2 = (p2_2 * o1).sin()\no3 = (p3 * (o2_1 + 2 * o2_2) + 9) ** 0.3\ny = 8 * o3\ny.backward()\n\nprint(p1.grad)\nprint(p2_1.grad)\nprint(p2_2.grad)\nprint(p3.grad)\n</code></pre> <pre><code>tensor(19.5745)\ntensor(1.0581)\ntensor(2.9195)\ntensor(-0.9488)\n</code></pre> <p>It seems to be right? No? You are welcome to verify. <code>:)</code></p> <p>With the following code (after pip installing torchviz):</p> <pre><code>from torchviz import make_dot\n\n# Visualize the computation graph\nmake_dot(y).render(\"computation_graph\", format=\"png\")\n</code></pre> <p>I've got the following picture:</p> Computation graph for y <p>For PyTorch models, one can use tensorboard to visualize the computation graphs (yes tensorboard supports also PyTorch). I was not able to achieve above on a \"stand alone\" tensor. It complained that the tensor does not have the attribute training, which is correct, yet probably not the intended outcome.</p> <p>While we haven't talked yet about neural networks, neighter about optimizers, and also not about data loaders, I'm bringing here a typical training loop, just to put in context autograd and backward.</p> <pre><code>...\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, data in enumerate(train_loader):\n        # Get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        # here the optimizer shall change the relevant model's parameters\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:  # Print every 100 mini-batches\n            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n            running_loss = 0.0\n\nprint(\"Finished Training\")\n</code></pre> <p>One needs to be a little aware when a calculation is part of a graph, and will result in the addition calculation of the gradients. There are a few reasons. Sometimes we're not training the model, but just making a use of its prediction capabilities (calculating gradients will waste resources). In other times we need to be aware that our calculation is outside the control of PyTorch autograd, for example, if we move stuff around for NumPy or using some external functionality / API. There are situations where we deliberty want some of the calculations to use autograd, while other not to. I bring here some related constructs, without giving the full context, yet we'll probably meet those constructs again later.</p> <pre><code># Disable autograd\nwith torch.no_grad(): # context manager\n    output = model(data)\n</code></pre> <pre><code>torch.autograd.set_grad_enabled(False)\n# Your code here\ntorch.autograd.set_grad_enabled(True)\n</code></pre> <pre><code>import torch\n\n# Create a tensor with gradient tracking enabled\nx = torch.randn(3, requires_grad=True)\n\n# Perform some operations\ny = x ** 2\n\n# Detach the tensor from the computation graph and convert to NumPy\nnumpy_array = y.detach().numpy()\n\nprint(numpy_array) # or maybe use matplotlib etc.\n# Not all plot's calculations should go all the way to to 'x' etc.\n</code></pre> <p>We can also disable the <code>requires_grad</code> for all, or some of, the relevant parameters of a model (the model's tensors). (will be shown elsewhere).</p>"},{"location":"embeddings/","title":"Embeddings","text":"<p>In my mind embeddings are just a synonym to features. For example, if we had:</p> <pre><code>linear = nn.Linear(in_features=5, out_features=10)\n</code></pre> <p>Then the input, x, for the call <code>linear(x)</code> should have in its last dimension exactly 5 float numbers that represent that entity.</p> <pre><code>input = torch.rand((30, 5))\noutput = linear(input)\n\nprint(output[0:2])\n</code></pre> <pre><code>tensor([[-0.7293, -0.1951,  0.5209,  0.5675, -0.1038,  0.5577,  0.5348, -0.4421,\n          1.0414,  0.5022],\n        [-0.6138, -0.2785,  0.1877,  0.5519, -0.1119,  0.3371,  0.4090, -0.5615,\n          1.0520,  0.3136]], grad_fn=&lt;SliceBackward0&gt;)\n</code></pre> <p>The five numbers, in the example above, can be, for example the following features about a person: age, num children, height, mark in math test, and lucky number; All represented as float numbers. We can also say that actually the output (10 numbers in the example above), are the representation of the person. While we've lost the \"interpretability\", this is still valid. We start with the original features, pass them through the linear layer, and we wind up with a continues (float numbers) representation that mixed everything together.</p> <p>Usually embeddings are a way to represent high-dimensional data in a lower-dimensional space, so not from 5 numbers to 10, but rather, as an example, from an image of 800 x 600 x 3 integer values to say 128 float numbers (so not just flatening the image into a long vector but more than that). Another example is taking variable length text (for example 1,000-2,000 characters), and returning 100 float numbers. Also representing categorical values, such as country name. So instead of country name, there will be say 4 float numbers. We could have also use one-hot-encoding in the example of the country name, meaning 1 for the relevant entry, and 0 everywhere else. Going even a step backwards, we could have just represent each country name by a unique number. What is the benefit of a float, dense, representation (as opposed to the one-hot-encoding sparsity)? With dense representation we can commit to a fix length vector. Also operations down the line, will be anyway with floats (for example the linear layer above). With dense vector, the representation is \"distributed\" among the entries. We should get some robustness to small changes in the values and we should get generality. The representation can and should be somewhat meaningful for the relevant task down the line. Making out of inputs of various shapes and nature, a vector of float numbers, is also refered as vectorization. Vectorization is then also the speed-up gained by operating on multiple entries at the same time using tensors operations, with the relevant linear-algebra packages / built-in support, we use.</p>"},{"location":"embeddings/#embedding-as-the-outputs-of-before-last-layer","title":"Embedding as the outputs of \"before last\" layer","text":"<p>How do we find the \"right\" embeddings for our inputs? One approach would be that finding the embeddings is part of the training. We start with the original pixel values of an image as an example, then down the line the various layers calculate values, and those are being improved with the training (when doing the backward pass). We can then say that the output of one or more of the layers is the embedding of the image. So the \"features\" were figured out automatically, and we can either examine those, or just accept them as are, and focus on the performance of the model as a whole.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(64*7*7, 128)\n        self.fc2 = nn.Linear(128, 10)  # Assuming 10 classes for classification\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 64*7*7)\n        x = F.relu(self.fc1(x))\n        embeddings = x  # This is the layer before the last\n        x = self.fc2(embeddings)\n        return x, embeddings\n\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(5):  # Train for 5 epochs\n    for data, target in train_loader:\n        optimizer.zero_grad()\n        output, _ = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\nmodel.eval()\nwith torch.no_grad():\n    for data, _ in train_loader:\n        _, embeddings = model(data)\n        print(embeddings.shape)  # This will print the shape of the embeddings\n        print(embeddings[1])  # Print the embeddings of the first image\n        break  # Just to demonstrate, we break after the first batch\n</code></pre> <pre><code>torch.Size([64, 128])\ntensor([ 0.0000,  1.1136,  0.0000,  0.0000,  5.8238,  0.0000,  5.3606,  0.0000,\n         0.0000,  0.0000,  3.7185,  0.0000,  0.0000,  3.8186,  0.0000,  7.1503,\n         0.0000,  0.0000,  0.1049, 12.1226,  0.0000,  0.0000,  0.0000,  0.0000,\n         6.4292,  0.0000,  0.0000,  8.9185,  0.0000,  1.5302,  0.0000,  7.0113,\n         0.0000,  5.6367,  5.8558,  0.1248,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  5.0979,  0.1330,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6371,  3.5945,  0.0000,\n         0.0000, 15.4092,  0.0000,  0.0000,  2.8716,  3.0890, 20.2494,  0.0000,\n         0.0000,  0.0000,  0.0000,  9.1179,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  7.2256,  2.6913,  0.0000,  0.1845, 11.2452,  0.0000,\n         6.1028,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2460,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  7.6430,  0.0000,  1.2241,\n         0.0000,  0.0000,  0.9366, 13.4370,  2.0505,  2.9676,  0.0000,  0.0000,\n         0.0000,  3.3068, 13.7987,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.2009,\n         0.0000,  0.0000,  0.0000,  2.5552,  0.0000,  0.0000, 11.6470,  0.0000])\n</code></pre> <p>We use CNN which is often done with images, it is like a filter that is \"passed\" across the image and generates a new \"image\", and so the same parameters of the filter are used in multiple places to try to see if the (same) signal is present there. So for example if you search for a human face, in the output \"image\" you can see high-likelihood in multiple places (maybe indeed there are multiple people in the image). We see above also <code>view</code> which is like <code>reshape</code> and is used to \"flatten\" the matrix into a vector. We see that <code>forward</code> returns not only the output, but also the value that we got in a previous layer (the input to <code>fc2</code>). We can do that when we have access to the class of the model and we know that we want to have the \"embeddings\". We then need to remember that the output is a tuple which includes also the embeddings. Also interesting to see the many zeros in the embeddings. The zeros can be thought of as turned-off features (things that are not present in the image). </p>"},{"location":"embeddings/#embedding-from-a-pre-trained-model","title":"Embedding from a pre-trained model","text":"<p>Another way, can be to rely on an existing, pre-trained model, to feed it our input, and to collect the embeddings either at the last layer (the pre-trained model), or from one of the layers that preceed the output. We then take the embeddings to be the input to our model. Hopefully the embedding \"logic\" is somewhat relevant to our task, and so our model can make use of its input (the embeddings) and learn the desired task.</p> <pre><code>from transformers import BertModel, BertTokenizer\n\n# Load pre-trained model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\n\n# Example text\ntext = \"Hello, how are you?\"\n\n# Tokenize the text\ninputs = tokenizer(text, return_tensors='pt')\n\n# Get the embeddings\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# The embeddings are in the last hidden state\nembeddings = outputs.last_hidden_state\n\nprint(embeddings.shape)\nprint(embeddings.squeeze())\n</code></pre> <pre><code>torch.Size([1, 8, 768])\ntensor([[-0.0824,  0.0667, -0.2880,  ..., -0.3566,  0.1960,  0.5381],\n        [ 0.0310, -0.1448,  0.0952,  ..., -0.1560,  1.0151,  0.0947],\n        [-0.8935,  0.3240,  0.4184,  ..., -0.5498,  0.2853,  0.1149],\n        ...,\n        [-0.2812, -0.8531,  0.6912,  ..., -0.5051,  0.4716, -0.6854],\n        [-0.4429, -0.7820, -0.8055,  ...,  0.1949,  0.1081,  0.0130],\n        [ 0.5570, -0.1080, -0.2412,  ...,  0.2817, -0.3996, -0.1882]])\n</code></pre> <p>We are using above the BERT model. We've used the <code>transformers</code> package from HuggingFace. From the original text \"Hello, how are you?\" we winded up with 8 tokens, and each got a 768 representation. How to go from there to the task, for example, does the text contain humour? If I understand correct, then from the text we got 6 tokens: [\"Hello\", \",\", \"how\", \"are\", \"you\", \"?\"], then two tokens where added, one at the start: [CLS], and another at the end [SEP]. We got 8 (token level) embeddings. From there we can either use only the first one (the one for the [CLS]), or we can consider the average as the \"sentense level\" embeddings:</p> <pre><code># Get the embeddings for the [CLS] token\ncls_embedding = embeddings[:, 0, :]\n\n# Or average the token embeddings\nsentence_embedding = embeddings.mean(dim=1)\n</code></pre> <p>One can then use above embeddings and train a classifier:</p> <pre><code>import torch.nn as nn\n\n\nclass HumorClassifier(nn.Module):\n    def __init__(self):\n        super(HumorClassifier, self).__init__()\n        self.linear = nn.Linear(768, 1)\n\n    def forward(self, x):\n        return torch.sigmoid(self.linear(x))\n\n\nclassifier = HumorClassifier()\n\n\n# ... obtain here texts and matching labels\n# ... obtain here embeddings for the texts\n\n\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(classifier.parameters(), lr=0.001)\n\n# Convert labels to tensor\nlabels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n\n# Training loop\nfor epoch in range(100):\n    optimizer.zero_grad()\n    outputs = classifier(embeddings)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n</code></pre> <p>We can also package the embedding part and even simplify some more, the process of getting the (say 768 floats) representation:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\n\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Sentences we want to encode. Example:\nsentence = ['This framework generates embeddings for each input sentence']\n\n# Sentences are encoded by calling model.encode()\nembedding = model.encode(sentence)\n</code></pre> <p>There is an alternative to above approach, which is refered to as fine-tuning. In the setting of fine-tuning we also start with an existing model (that was pre-trained), but then we train it entirely, or partly (some of the layers), on the target task (transfer learning). This is a bit less related to embeddings, but I wanted to bring it here in the context of using pre-trained models.</p>"},{"location":"embeddings/#embeddings-from-auto-encoder","title":"Embeddings from Auto-Encoder","text":"<p>Embedding can come from an \"auto-encoder\" process, in which we try to \"compress\" the information and then see if we can reco0nstract the original input. This can be achieved from unsupervised data, and thus allows us to use big quantities to examples (that we're collected for example from the WWW).</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n# Define the autoencoder class\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(28 * 28, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 12),\n            nn.ReLU(),\n            nn.Linear(12, 6)  # Bottleneck layer\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(6, 12),\n            nn.ReLU(),\n            nn.Linear(12, 64),\n            nn.ReLU(),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 28 * 28),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = F.sigmoid(x)\n        x = self.decoder(x)\n        return x\n\n# Load the dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\n# Initialize the model, loss function, and optimizer\nmodel = Autoencoder()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(10):  # Train for 5 epochs\n    for data in trainloader:\n        inputs, _ = data\n        inputs = inputs.view(-1, 28 * 28)  # Flatten the images\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, inputs)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')\n\nprint(\"Training complete!\")\n</code></pre> <pre><code>Epoch [1/5], Loss: 0.9208\nEpoch [2/5], Loss: 0.9284\nEpoch [3/5], Loss: 0.9289\nEpoch [4/5], Loss: 0.9180\nEpoch [5/5], Loss: 0.9259\nEpoch [6/5], Loss: 0.9170\nEpoch [7/5], Loss: 0.9078\nEpoch [8/5], Loss: 0.9036\nEpoch [9/5], Loss: 0.9026\nEpoch [10/5], Loss: 0.9000\nTraining complete!\n</code></pre> <p>Above seems to make some progress; smaller and smaller loss. The embeddings that I had in mind, are the output after the encoder (6 floats).</p> <p>There are multiple other ways to achieve meaningful embeddings that are a result of some smart training, usually from unsupervised dataset (for example collecting texts from the web and learning what is the next word). Assume you maintain a dictionary. You limit the dictionary to 100,000 words, maybe reserving one entry for all other words. Then each entry has a matching index. So for example 'hello' gets 7 and 'world' gets 88, so 'hello world' is represepted by 7, 88. Then this is often first converted to one-hot-encoding (sparse representation), but the next layer may learn dense representation, such that finaly 'dog' is closer to 'cat' than to 'airplaine' in the space of the embeddings, and wrt some distance measure.</p>"},{"location":"embeddings/#embedding-for-a-categorical-column","title":"Embedding for a categorical column","text":"<p>Below I want to bring an example of \"learning\" dense embeddings with PyTorch for a categorical column. If I understand correct, there is no special support for maintaining the dictionary and coming up with appropriate index. One uses regular Python for that. Once you have the indices, PyTorch helps with the assembly of one-hot-encoding and from there the  dense representation and the training process.</p> <pre><code>import pandas as pd\n\n# Example categorical column\ndata = {'category': ['cat', 'dog', 'bird', 'cat', 'bird', 'dog']}\ndf = pd.DataFrame(data)\n\n# Create a unique index for each category\ndf['category_index'] = df['category'].astype('category').cat.codes\nprint(df)\n</code></pre> <pre><code>  category  category_index\n0      cat               1\n1      dog               2\n2     bird               0\n3      cat               1\n4     bird               0\n5      dog               2\n</code></pre> <pre><code>import torch\nimport torch.nn.functional as F\n\n# Convert category indices to one-hot encoding\nnum_classes = df['category_index'].nunique()\none_hot = F.one_hot(torch.tensor(df['category_index']).to(torch.int64), num_classes=num_classes)\nprint(one_hot)\n</code></pre> <pre><code>tensor([[0, 1, 0],\n        [0, 0, 1],\n        [1, 0, 0],\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, 0, 1]])\n</code></pre> <p>Now look at the following and imagine that, we can train this model for a few iterations on our task, and get useful embeddings mapping from indices, that shall enable the learning of the task.</p> <pre><code>import torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self, num_classes, embedding_dim):\n        super(SimpleModel, self).__init__()\n        self.embedding = nn.Embedding(num_classes, embedding_dim)\n        self.fc = nn.Linear(embedding_dim, 1)  # Example output layer\n\n    def forward(self, x):\n        embedding = self.embedding(x)\n        x = self.fc(embedding)\n        return x, embedding\n\n\n# Instantiate and test the model\nmodel = SimpleModel(num_classes=num_classes, embedding_dim=4)\noutput, embeddings = model(torch.tensor(df['category_index']).to(torch.int64))\nprint(output)\nprint()\nprint(embeddings)\n</code></pre> <pre><code>tensor([[-1.1242],\n        [ 0.0574],\n        [-0.7675],\n        [-1.1242],\n        [-0.7675],\n        [ 0.0574]], grad_fn=&lt;AddmmBackward0&gt;)\n\ntensor([[-0.5776,  0.3355,  2.3085, -0.1005],\n        [-0.0302,  0.7318, -1.4959, -0.7707],\n        [-1.1924,  0.0649,  1.2808,  2.1736],\n        [-0.5776,  0.3355,  2.3085, -0.1005],\n        [-1.1924,  0.0649,  1.2808,  2.1736],\n        [-0.0302,  0.7318, -1.4959, -0.7707]], grad_fn=&lt;EmbeddingBackward0&gt;)\n</code></pre> <p>Above, the <code>nn.Embedding</code> layer contains the one-hot-encoding logic, so its input is actually the index as an <code>torch.int64</code> value.</p>"},{"location":"neural_networks/","title":"Neural Networks with PyTorch","text":"<p>We're already seen tensors. Imagine a calculation, where at one side you have inputs, or readings from a sensor etc. At the other side of the calculation (the results) is a prediction, or a decision, or also some other desired output (such as a text describing in plain English what we see in the input image). The claim of neural networks (NN), is that such setting can be realized with a tensor or often multiple tensors, operating on the input(s), with potentially more mathematical operations on the way, and the output shall indeed be what we need it to be.</p> <p>Compare this way of thinking about a challenge to be automated, to traditional rule-based systems. Just a long, complicated and composed as needed, but with NN, just a feed-forward calculation. Compared above to a series of (nested) if-then-else and relevant logic. To be really honest, the calculations of a NN involve functions that \"contain\" if-like logic, such as .abs, .clip, and more. Also the rule-based solutions start with processing the inputs and extracting statistics and \"features\", such as: \"number of words on avarage in each sentence\", or \"total sum of purchases that month\", etc.</p> <p>The magic for a NN happens when the tensors contain the right values in each of their entries. All those values in the tensors, work together, as to \"extract statistics and features\" from the input(s), pass those throught the \"if-like\" logic, and output the desired result. Unlike traditional programming, filling the right values in the tensors, is done by a training algorithm (optimization) rather than by setting those following a clever analytics.</p> <p>While we could work with bare tensors, PyTorch has higher level construct nn.Module.</p> <pre><code>import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(5, 10)\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.tanh(x)\n        x = self.fc2(x)\n        return F.tanh(x)\n\n\nmodel = MyModel()\n\nout = model(torch.rand(5))\nprint(out)\nprint(out.shape)\n</code></pre> <pre><code>tensor([0.0552], grad_fn=&lt;TanhBackward0&gt;)\ntorch.Size([1])\n</code></pre> <p>Above, we've derived a new class from nn.Module. Our NN model is a nn.Module. We'll have a couple of attributes to each instance, that is in the example fc1 and fc2. In the constructor, those are just created and saved for later. nn.Linear is basicly two tensors. In the example of fc1, one of size [10, 5] and the other of size [10] (bias). The operation implied by nn.Linear is basically matmul bewteen the first tensor and the input we then add the bias tensor. fc stands for \"fully connected\", this name is used often for nn.Linear. The internal modules are the \"layers\" of the neural network.</p> <pre><code>model.fc1.state_dict()\n</code></pre> <pre><code>OrderedDict([('weight', tensor([[-0.1497, -0.2375,  0.1056, -0.3654,  0.3629],\n        [ 0.0529,  0.3939, -0.0946,  0.3917,  0.3135],\n        [-0.3274, -0.0793,  0.1388,  0.2727, -0.3741],\n        [ 0.2795, -0.2330, -0.0990,  0.4247,  0.4342],\n        [ 0.1221,  0.2291, -0.1657,  0.4082,  0.2261],\n        [ 0.2819,  0.4004, -0.4413,  0.0400,  0.4329],\n        [ 0.2175,  0.0362,  0.2990,  0.3109, -0.2700],\n        [ 0.2692, -0.3269, -0.3591, -0.0944, -0.2211],\n        [ 0.2267, -0.4393,  0.3136,  0.2568,  0.0721],\n        [-0.3871, -0.3472, -0.1821, -0.4447,  0.2364]])), ('bias', tensor([-0.1682, -0.3213,  0.2310, -0.2638,  0.4306,  0.0138, -0.0102, -0.2979,\n        -0.2500, -0.3362]))])\n</code></pre> <p>The values of the tensors where randomly initialized.</p> <p>nn.Linear is by itself an nn.Module:</p> <pre><code>isinstance(model.fc1, nn.Module)\n</code></pre> <p><code>True</code></p> <p>It is nice that we compose nn.Module of other modules, as we can define custom \"building blocks\" and use them later as part of a NN model (imagine you want to repeat a construct a few times). It makes everything more \"modular\".</p> <p>In forward we make use of the internal layers on the module. We pass the input throught them and let them do their calculations. In the example above we broke the linearity between fc1 and fc2 using tanh. tanh does not have parameters, and so we did not need to allocate attributes in the constructor, but rather using it from torch.nn.functional, which is a collection of functions. Another name for those non-linearity functions is activation functions.</p> <p>Note that when we use the model above <code>out = model(torch.rand(5))</code>, we've \"called\" the model which in turns calls forward. We passed a single input, this was the expected input, a vector of 5 elements. We can also pass a mini-batch of inputs <code>out = model(torch.rand(5).unsqueeze(0))</code>. The output in the first case is <code>torch.Size([1])</code> and in the mini-batch one is <code>torch.Size([1, 1])</code>. I think we've have single element / mini-batch for free as of the broadcast capabilities of tensors. One can also have a mini-batch of mini-batches (if makes sense) etc.</p> <p>Note that the tensors where defined as ones that require autograd. This means that when we use the result of the network, potentially calculating the loss from the result and some desired output, we can backward the gradient to all relevant calculations that were used for the loss and internally in the forward implementation.</p> <p>Printing the model will yield something as follows.</p> <pre><code>MyModel(\n  (fc1): Linear(in_features=5, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=1, bias=True)\n)\n</code></pre> <p>Note that the graph is aware of the names of the attributes as we named them in the initialization. With tensorboard, I got the following diagram:</p> <pre><code># make sure first to \"pip install tensorboard\"\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\nwriter.add_graph(model, torch.rand(5))\nwriter.close()\n# now open tensorboard --logdirs=runs and check for the graph of the latest run\n</code></pre> Computation graph for model <p>One can add the non-linearity also to the graph with a bit more attributes to the model:</p> <pre><code>class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(5, 10)\n        self.nl1 = nn.Tanh()\n        self.fc2 = nn.Linear(10, 1)\n        self.nl2 = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.nl1(x)\n        x = self.fc2(x)\n        return self.nl2(x)\n\n\nmodel = MyModel()\n</code></pre> <p>nn.Module brings some more goodies. For example, while we've added attributes to our class, those \"parameters\" where also noted, and are currently available (see above usage of <code>state_dict</code>).</p> <pre><code>list(model.parameters())\n</code></pre> <pre><code>[Parameter containing:\n tensor([[ 0.2207,  0.1466, -0.3568, -0.2860, -0.0585],\n         [-0.1582,  0.1217, -0.0859,  0.1306,  0.1662],\n         [-0.0158,  0.4182, -0.3181,  0.0978, -0.1387],\n         [ 0.1375,  0.3619,  0.2441, -0.3241, -0.0193],\n         [-0.1960,  0.4269, -0.2684,  0.0177,  0.4463],\n         [-0.1160, -0.0506, -0.2181,  0.1954,  0.2758],\n         [-0.3748,  0.1646, -0.1075, -0.1940, -0.4468],\n         [ 0.3785,  0.1593, -0.3964, -0.0366, -0.2333],\n         [ 0.1707, -0.0325, -0.0823,  0.0911, -0.3582],\n         [ 0.1998, -0.1256,  0.2642, -0.2502,  0.2250]], requires_grad=True),\n Parameter containing:\n tensor([ 0.0451,  0.3004, -0.0041,  0.4164, -0.0115, -0.3451, -0.3801, -0.1956,\n         -0.1690,  0.3703], requires_grad=True),\n Parameter containing:\n tensor([[ 0.1877, -0.3158,  0.0916,  0.1542,  0.1127,  0.0930,  0.1966,  0.2425,\n           0.0706,  0.2424]], requires_grad=True),\n Parameter containing:\n tensor([0.2331], requires_grad=True)]\n</code></pre> <p>Values above may differ as I ran the snippet in another opportunity. The ability to have the <code>state_dict</code> of a model will help us when we want to save a copy of a model on disk. If we fail to save the parameters, we'll need to train and verify each time we need such a model. The real value of a model is one that was trained and gave us good results on unseen dataset. <code>parameters</code> will be useful for us when we want to select what parameters to train / fine-tune.</p> <p>Please note that a <code>nn.Module</code> is aware only of other <code>`nn.Module</code> that is an attribute of the object. Not sure how this wonderful capability was achieved in the first place, yet we do need to be aware of it. Let's try the following:</p> <pre><code>class MyModel(nn.Module):\n    def __init__(self, num_layers=3):\n        super(MyModel, self).__init__()\n        self.layers = []\n        in_features = 5\n        out_features = 10\n        for _ in range(num_layers - 1):\n            self.layers.append(nn.Linear(in_features, out_features))\n            in_features = out_features   \n        self.layers.append(nn.Linear(in_features, 1))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n            x = F.tanh(x)\n        return x\n\n\nmodel = MyModel()\nmodel.state_dict()\n</code></pre> <p><code>OrderedDict()</code></p> <p>The funny part, is that above mostly do work:</p> <pre><code>model(torch.rand(5))\n</code></pre> <p><code>tensor([-0.3304], grad_fn=&lt;TanhBackward0&gt;)</code></p> <p>Easy solution for above is to use <code>nn.ModuleList</code></p> <pre><code>class MyModel(nn.Module):\n    def __init__(self, num_layers=3):\n        super(MyModel, self).__init__()\n        layers = []\n        in_features = 5\n        out_features = 10\n        for _ in range(num_layers - 1):\n            layers.append(nn.Linear(in_features, out_features))\n            in_features = out_features   \n        layers.append(nn.Linear(in_features, 1))\n        self.layers = nn.ModuleList(layers)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n            x = F.tanh(x)\n        return x\n\n\nmodel = MyModel(2)\nmodel.state_dict()\n</code></pre> <pre><code>OrderedDict([('layers.0.weight',\n              tensor([[ 0.3956, -0.3880,  0.4201, -0.1354, -0.1536],\n                      [ 0.1374,  0.0503,  0.0612,  0.0231,  0.1588],\n                      [-0.0798, -0.2296, -0.0735,  0.3649,  0.3362],\n                      [ 0.2273,  0.3117,  0.1105, -0.0899,  0.3984],\n                      [-0.2068, -0.1243, -0.0746, -0.0251,  0.0485],\n                      [-0.1613, -0.2822, -0.2108, -0.1591,  0.0344],\n                      [ 0.0259, -0.3709, -0.3758, -0.1356,  0.3477],\n                      [-0.3030,  0.0275, -0.3157,  0.1007,  0.2047],\n                      [-0.0222,  0.0705,  0.2005, -0.3808, -0.0279],\n                      [ 0.0557,  0.2277,  0.3824, -0.3172,  0.1456]])),\n             ('layers.0.bias',\n              tensor([ 0.2578, -0.0806,  0.1190,  0.2998,  0.2751,  0.1743,  0.2498, -0.1855,\n                      -0.3150,  0.1348])),\n             ('layers.1.weight',\n              tensor([[ 0.0866, -0.0983, -0.1061, -0.1938,  0.2220, -0.0435, -0.1673,  0.2343,\n                       -0.1903, -0.2391]])),\n             ('layers.1.bias', tensor([0.0450]))])\n</code></pre> <p>One can also use <code>nn.Sequential</code> which includes also the cascading loop.</p> <pre><code>class MyModel(nn.Module):\n    def __init__(self, num_layers=3):\n        super(MyModel, self).__init__()\n        self.layers = nn.Sequential()\n        in_features = 5\n        out_features = 10\n        for _ in range(num_layers - 1):\n            self.layers.extend([nn.Linear(in_features, out_features), nn.Tanh()])\n            in_features = out_features   \n        self.layers.extend([nn.Linear(in_features, 1), nn.Tanh()])\n\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nmodel = MyModel()\n</code></pre> Sequential (as shown on tensorboard if you add the graph of the model)"},{"location":"pytorch_vs_numpy/","title":"PyTorch as a Linear-Algebra support package","text":"<p>PyTorch provides utilities to manipulate numbers with linear-algebra style logic and operations. A data point with two values, for example age and height, can be represented by a vector with two items, where the first entry is the age, and the second entry is the height.</p> <pre><code>import torch\n\n\n# first entry age in years, second entry height in centimeters\nperson_details = torch.tensor([18, 172.4])\n\nperson_details\n</code></pre> <p><code>tensor([ 18.0000, 172.4000])</code></p> <p>Why not a dataclass (or just a Python class)? Why not a Python dict? It is still a good practice to use more structured data types in your program and databases. At one stage to communicate the data to machine learning; linear models, neural networks, etc., we shall need probably to work with this kind of representation, meaning vectors, matrices, and higher dimensions tensors, with all entries of the same (numeric) type.</p> <p>If you are familiar with DataFrames from Pandas, Polars, or Spark, or if you have already used tabular data with ML libraries such as Scikit-Learn, this seems to be a step backwards. Yet think of streams of images from cameras (a video or so), embedding for variable-length texts, sound, the feed from sensors, etc. For more flexibility with the inputs and the outputs, getting used to tensor representation will help us with addressing those challenges. We'll need to find the right combination of \"human facing\" abstractions and \"neural network facing\" representation, and where to do the back and forward translations.    </p> <p>So a vector is tensor with one dimention. To continue with above example, we can have the details for more than one person, in a matrix. Each row will represent another person (another data point.).</p> <pre><code>..\n\n# first entry age in years, second entry height in centimeters\nsubjects_details = torch.tensor([\n    [18, 172.4],\n    [51, 169.2],\n    [46, 164.5],\n])\n\nsubjects_details\n</code></pre> <pre><code>tensor([[ 18.0000, 172.4000],\n        [ 51.0000, 169.2000],\n        [ 46.0000, 164.5000]])\n</code></pre> <p>If you are saying at this stage, \"hey, this is just like NumPy\", then you are right. Tensors in PyTorch are like arrays in NumPy. The functionality of NumPy is mostly available also with PyTorch, and even the inteface of PyTorch was inspired by NumPy and is very similar. So why yet another implementation?</p> <p>PyTorch's tensors bring additional functionalities that are not found in NumPy. Trying to add the functionalities, we'll describe those later, to NumPy, would have slowed the process and Meta, the developers behind PyTorch wanted to run fast. Also NumPy have some target audience and relevant optimizations, that are different from what the creators of PyTorch needed. PyTorch is optimized towards the usage with neural networks. NumPy is more memory optimized.</p> <p>There are still many ideas in NumPy worth getting to know, that are also relevant while using PyTorch. For example the concept of vectorization, which is, instead of looping over the entries and applying an operation to each entry at a time, issue the operation on the whole set of entries at once, where possible, letting the implementation by the package use hardware and other tricks to apply \"at the same time\" to all entries, or to batches of inputs, transparently to the caller (ex. instead of applying 1,000 times applying 10 times for each batch/slice of size 100). </p> <p>An example of things you can do with PyTorch (and with Numpy) is the following: let's say we want to calculate the BMI ( Body Mass Index) for people, based on the following formula:</p>  BMI = {mass(kg) \\over height^2(m)}  <pre><code>..\n\n# first entry age in years, second entry height in centimeters,\n# and just added the mass in kilograms.\nsubjects_details = torch.tensor([\n    [18, 172.4, 62.1],\n    [51, 169.2, 66.0],\n    [46, 164.5, 72.0],\n])\n\nsubjects_BMI = (\n    subjects_details[:, 2] / (subjects_details[:, 1] / 100.0) ** 2\n)\n\nsubjects_BMI\n</code></pre> <p><code>tensor([20.8938, 23.0538, 26.6073])</code></p> <p>What we did there? We picked the third column (index 2) and devided it (element wise) with the second column (index 1), after the last was first converted to meters, and then raised to the power of 2. If I read the results correct, then the third person is a bit overweight.</p> <p>A side note. Is NumPy obsolete? Can we forget about NumPy, or never bother to learn it if did not play with it yet? My answer will be that at the moment it is still around and is well supported and integrated by and with many packages. Also I believe it is a dependancy of PyTorch (probably not making use of NumPy, yet as of back-and-forward conversions.). It is very easy to initialize a PyTorch tensor from a NumPy array and vice versa. So keep learning PyTorch, and use NumPy if and where needed.</p> <p>It is a good practice to verify the type of a Python object, when in doubt.</p> <pre><code>type(subjects_details)\n</code></pre> <p><code>torch.Tensor</code></p> <p>To get a bit more information with tensors, use also the following:</p> <pre><code>subjects_details.type()\n</code></pre> <p><code>'torch.FloatTensor'</code></p> <pre><code>subjects_details.dtype\n</code></pre> <p><code>torch.float32</code> (the elements are all of torch.float32 type)</p> <pre><code>subjects_details.shape\n</code></pre> <p><code>torch.Size([3, 3])</code> (3 rows and 3 columns right after we've added also the mass)</p> <p>Tensors are very useful also to represent images.</p> <pre><code>..\nimg = torch.tensor([\n    [0, 1, 1, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 1],\n    [0, 1, 1, 0],\n])\n\nimg\n</code></pre> <pre><code>tensor([[0, 1, 1, 0],\n        [1, 0, 0, 1],\n        [1, 0, 0, 1],\n        [0, 1, 1, 0]])\n</code></pre> <pre><code>img.type()\n</code></pre> <p><code>'torch.LongTensor'</code></p> <p>Above the implementation of <code>torch.tensor</code> noted that all the entries are integers and so we got a <code>LongTensor</code>. If you want to have floats in the first place, use <code>tensor.Tensor</code> instead:</p> <pre><code>img = torch.Tensor([\n    [0, 1, 1, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 1],\n    [0, 1, 1, 0],\n])\n\nimg\n</code></pre> <pre><code>tensor([[0., 1., 1., 0.],\n        [1., 0., 0., 1.],\n        [1., 0., 0., 1.],\n        [0., 1., 1., 0.]])\n</code></pre> <p>Alternatively you can convert an existing tensor to another type, by a match operation, or explicitly using <code>to</code>:</p> <pre><code>img = torch.tensor([\n    [0, 1, 1, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 1],\n    [0, 1, 1, 0],\n])\n\nimg.to(torch.float32)\n</code></pre> <pre><code>tensor([[0., 1., 1., 0.],\n        [1., 0., 0., 1.],\n        [1., 0., 0., 1.],\n        [0., 1., 1., 0.]])\n</code></pre> <p>Images often have multiple channels (ex. RGB). With PyTorch the channel should often come before the height and the width, so if you happen to have HWC and you want to \"change\" it into CHW, you can do the following:</p> <pre><code>img = ... # a tensor representing an image, given in HWC format.\nimg = img.permute([2, 0, 1]) # let's have it in CHW format.\n</code></pre> <p>To verify above snippet of code, I've ended up creating first CHW image and then converting it to HWC (just to convert it back):</p> <pre><code>chw_img = img.unsqueeze(0).repeat(3, 1, 1)\nchw_img\n</code></pre> <p>unsqeeuze adds a dimention. repeat in this example \"inflates\" it from B/W to RGB (three channels).</p> <pre><code>tensor([[[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]]])\n</code></pre> <pre><code>chw_img[1] *= 2\nchw_img[2] *= 3\nchw_img\n</code></pre> <pre><code>tensor([[[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 2, 2, 0],\n         [2, 0, 0, 2],\n         [2, 0, 0, 2],\n         [0, 2, 2, 0]],\n\n        [[0, 3, 3, 0],\n         [3, 0, 0, 3],\n         [3, 0, 0, 3],\n         [0, 3, 3, 0]]])\n</code></pre> <p>Lastly let's show what we wanted, so if the channel is the last dimension HWC:</p> <pre><code>hwc_img = chw_img.permute(1, 2, 0)\nhwc_img[:, :, 2]\n</code></pre> <pre><code>tensor([[0, 3, 3, 0],\n        [3, 0, 0, 3],\n        [3, 0, 0, 3],\n        [0, 3, 3, 0]])\n</code></pre> <p>Above demonstrates what the third channel (Blue?) looks like. Now we can change the channel to be in the first dimension (as usually is assumed with PyTorch).</p> <pre><code>hwc_img.permute(2, 0, 1)\n</code></pre> <pre><code>tensor([[[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 2, 2, 0],\n         [2, 0, 0, 2],\n         [2, 0, 0, 2],\n         [0, 2, 2, 0]],\n\n        [[0, 3, 3, 0],\n         [3, 0, 0, 3],\n         [3, 0, 0, 3],\n         [0, 3, 3, 0]]])\n</code></pre> <p>It is usually a good practice to operate on tensors and get new tensors as a result. This is more \"functional\" in the sense that code and functions that we run do not have side effects on the inputs, and shall return the same results (subject to implicit randomness), if called again with the same inputs. If we do need to save space, we can opt in to \"in place\" operations.</p> <p>A very useful functionality of tensor is reshape. Imagine a 2d image. Its pixels' values may be stored on disk in a sequential mannner. When we load the image, we are starting with a 1d-tensor. Then maybe we want to process the image with its \"georgaphy\" in mind (cropping for example).</p> <pre><code>img_vec = torch.arange(9)\nprint(img_vec.shape)\nimg_mat = img_vec.reshape(-1, 3)\nprint(img_mat.shape)\n</code></pre> <pre><code>torch.Size([9])\ntorch.Size([3, 3])\n</code></pre> <p>Above we've suggested that there are 3 columns, and let PyTorch do the math regarding the rows (this was the -1).</p> <pre><code>img_mat\n</code></pre> <pre><code>tensor([[0, 1, 2],\n        [3, 4, 5],\n        [6, 7, 8]])\n</code></pre> <p>When makes sense, PyTorch attempts to avoid moving memory from place to place, but rather only keep \"management\" information regarding the shape. This however means that there is only one \"place\":</p> <pre><code>img_mat[-1, -1] = 88\nimg_vec\n</code></pre> <pre><code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7, 88])\n</code></pre> <p>You may see in places the usage of .view() which is like .reshape() yet may fail when the memory is not continous. So if you want to make sure the memory is continous, use .view(), if you want it just to work, use .reshape().</p> <p>We've seen above element wise operations, for example when we've computed BMI. PyTorch, as does NumPy, supports also linear algebra style vector and higher-order tensors operations, such as multiplication.</p> <pre><code>tensor1 = torch.randn(3)\ntensor2 = torch.randn(3)\nres = torch.matmul(tensor1, tensor2)\nprint(res)\nprint(res.size())\nprint(res.item())\n</code></pre> <pre><code>tensor(-1.0853)\ntorch.Size([])\n-1.0853235721588135\n</code></pre> <p>We got above a 0-dimensions tensor (a scalar). In order to take the single value out of the tensor, we've used .item().</p> <p>Note that the mathematical rules dictate the the dimensions in above example should have been [1, 3] and [3, 1], yet .matmul figures out what we want to do there.</p>"},{"location":"pytorch_vs_numpy/#broadcasting","title":"Broadcasting","text":"<p>When we need element-wise operations, such as addition, the tensors are supposed to be of the same size, that is same dimensios, and same number of elements at each dimension. But we've already seen a division by scalar, which means that every element is diveded by the same scalar, or you can also think of it that a conceptual tensor is created, and the scalar is broadcasts to all elements of the new tensor, and then we have the element-wise division. Broadcasting is even more general. If you have two tensors, with different sizes, it can still work, if it is possible to \"understand\" what is meant; starting from the last dimension, if it is equal among the tensors, we move to the next one. If one of the dimentions is 1, then we assume all entries of the conceptual tensor are the same. If we ran out of dimension in one tensor, we can assume we have there 1 as above.</p> <p>Here are some examples that should work:</p> <pre><code>[3, 4, 5] and [3, 1, 5]\n[1, 2, 2] and [2, 2, 2]\n[2, 2] and [2, 2, 2]\n</code></pre> <p>And here are some examples that should probably don't work:</p> <pre><code>[3, 3] and [3, 6]\n[2, 2] and [3, 2]\n</code></pre> <p>If we did not have the support of broadcasting, we could have just do as above with unsqueeze, squeeze, repeat, etc. However it makes our life a bit easier, and potentially saves memory and computations.</p> <pre><code>img = torch.tensor([\n    [0, 1, 1, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 1],\n    [0, 1, 1, 0],\n])\n\nimg * torch.tensor([[[1]], [[2]], [[3]]]) # [4, 4] and [3, 1, 1]\n</code></pre> <pre><code>tensor([[[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 2, 2, 0],\n         [2, 0, 0, 2],\n         [2, 0, 0, 2],\n         [0, 2, 2, 0]],\n\n        [[0, 3, 3, 0],\n         [3, 0, 0, 3],\n         [3, 0, 0, 3],\n         [0, 3, 3, 0]]])\n</code></pre>"},{"location":"pytorch_vs_numpy/#support-for-gpu","title":"Support for GPU","text":"<p>Tensors can be allocated on the GPU's memory, if you have such. Relevant operations on the tensors will run there (on the GPU), often faster. Operations between tensors need to be done when both tensors are located on the same device. If you detect that you have a GPU, and you train a model, putting the parameters of the model on the GPU, the training and evaluation data, should also be at the time on the GPU. Data is often loaded from the disk into the main memory, and then mini-batches are copied to the GPU's memory, used for an iteration, and make space for the next mini-batches, that are waiting either in memory or still on the disk. NumPy as far as I know does not have support for GPU. There is another NumPy like package called Numba, that apparently does support GPU. I haven't played with Numba yet.</p> <pre><code>torch.cuda.is_available()\n</code></pre> <pre><code>True\n</code></pre> <p>Above was shown on Colab. If like me you don't have GPU in your private environment, consider using Colab.</p> <pre><code>tensor1 = torch.randn(3)\ntensor1.device\n</code></pre> <p><code>device(type='cpu')</code></p> <pre><code>tensor1_cuda = tensor1.to(\"cuda\")\ntensor1_cuda.device\n</code></pre> <p><code>device(type='cuda', index=0)</code></p> <p>Just to iterate, operations between tensors that are located on different devices, are not supposed to work:</p> <pre><code>...\n\ntensor1 = torch.randn(3)\ntensor2 = torch.randn(3).to(\"cuda\")\ntensor1 + tensor2\n</code></pre> <pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-2-5076b2d4339c&gt; in &lt;cell line: 3&gt;()\n      1 tensor1 = torch.randn(3)\n      2 tensor2 = torch.randn(3).to(\"cuda\")\n----&gt; 3 tensor1 + tensor2\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n</code></pre> <p>If you do want to add as an example two tensors that are on different device, you need first to move one of them to the same device where the other is. Remember that the GPU is fast, while the CPU (the relevant memory) is potentially bigger, but also when needed, backed by even larger space transparently (the operating system's virtual memory and the disk). </p> <p>There is more to PyTorch of course, and also related to tensors. We'll talk about auto grad next.</p>"},{"location":"training/","title":"Training","text":"<p>We already have a model, which knows to feed-forward some input(s) in a specific size (shape) and return specific size output(s). We've seen an easy and modular way to define those, in a way that keeps track of all our parameters, and that calculate for us partial derivatives, needed for gradient descent. We need now a training dataset. A dataset can be given for example as a tensor with the appropriate dimensions and size. The first dimension can be the instance (data item), so that we actually get a batch of our examples. For simplicity of the discussion, let's assume we're in supervised setting, that means that for every entry in the batch we have also a label.</p> <pre><code>import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ndef get_dataset():\n    X = torch.rand(20, 5)\n    y = torch.matmul(X, torch.tensor([0.3, 4.0, 0.2, 2.5, 18.0])) + 17.0\n    return X, y\n\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(5, 1)\n\n    def forward(self, x):\n        return self.fc1(x)\n\n\nX, y = get_dataset()\nprint(f'{X.shape=}', f'{y.shape=}')\nmodel = MyModel()\n\nlr = 1e-3 # learing rate\n\nfor epoch in range(10):\n    output = model(X)\n    assert output.shape == torch.Size((20, 1))\n    loss = F.mse_loss(output.squeeze(), y)\n    # the target (y) is the second parameter\n    # (yes in scikit-learn it is the other way around)\n    print(f'{loss=}')\n    for parameter in model.parameters():\n        parameter.grad = None\n    loss.backward() # send the grads to the model's parameters\n    for parameter in model.parameters():\n        parameter.data -= lr * parameter.grad\n</code></pre> <pre><code>X.shape=torch.Size([20, 5]) y.shape=torch.Size([20])\nloss=tensor(880.1790, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(872.0485, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(863.9950, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(856.0178, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(848.1162, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(840.2894, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(832.5369, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(824.8579, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(817.2516, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(809.7174, grad_fn=&lt;MseLossBackward0&gt;)\n</code></pre> <p>Note that we assumed that the whole dataset can fit into our device's memory, be it the \"cpu\" or \"cuda\" (for GPU). Often with gradient descent we use mini-batches and each epoch is actually a loop itself where on each iteration a single mini-batch is feed-forwarded and then the lesson regarding the loss is learned. The main reason for working with mini-batches is memory limitations, and there might some others. PyTorch has nice support for datasets and data loaders, that shall be shown shortly.</p> <p>The other element I want to bring up is that I've used above the grad of a parameter to update its data. On each epoch, I've updated wrt to the gradients, but took from it only <code>lr</code>. There are even better methods to optimize based on the gradients, potentially remembering what we have done in previous rounds, and to be \"consistent\" in the updates, or to keep the momentum. There are other smarter things to do, including to adapt the <code>lr</code> with time, or to have a different learning rate for different parameters. A lot of this logic is supported with optimizers. We'll see that also later. But first let's look datasets and data loaders.</p>"},{"location":"training/#datasets-and-data-loaders","title":"Datasets and Data Loaders","text":"<pre><code>import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\n\n# Define a transform to convert the data to tensor\ntransform = transforms.ToTensor()\n\n# Download and load the training data\ntrain_data = datasets.FashionMNIST(\n    root='data',\n    train=True,\n    download=True,\n    transform=transform\n)\n\n# Download and load the test data\ntest_data = datasets.FashionMNIST(\n    root='data',\n    train=False,\n    download=True,\n    transform=transform\n)\n\n# Create data loaders\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n\n# Visualize some training data\ndef show_images(images, labels):\n    plt.figure(figsize=(10, 10))\n    for i in range(25):\n        plt.subplot(5, 5, i+1)\n        plt.imshow(images[i].numpy().squeeze(), cmap='gray')\n        plt.title(labels[i].item())\n        plt.axis('off')\n    plt.show()\n\n# Get a batch of training data\nimages, labels = next(iter(train_loader))\nshow_images(images, labels)\n</code></pre> <pre><code>0.9%\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n100.0%\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n100.0%\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n\n14.8%\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n100.0%\n100.0%\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n</code></pre> Samples from fashion MNIST <p>Above example we have made a usage of a sample dataset found in the useful Python package <code>torchvision</code>, that is <code>FashionMNIST</code>. The nice thing about using a dataset is that it hides details such as where the files are (already loaded to memory, local disk, on the Internet, etc.). Also we had two such datasets. One for training and another for testing. And we had a chance to add transformations (which then applied be item). In this case we've made sure the items are given to us as PyTorch tensors. We did not access the datasets directly but rather used a utility class <code>DataLoader</code> that allows us to define the batch (mini-batch) size, as well as wheater we want to shuffle the entries first. Shuffeling may be beneficial for the training, but is not important for the test. Out of the 64 images that we get in the first mini-batch, we show above the first 5x5 images (and their supervised labels).</p> <p>One often defines their own dataset.</p> <pre><code>class CustomImageDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n        image = io.imread(img_path)\n        label = int(self.annotations.iloc[idx, 1])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n\nfrom torchvision import transforms\n\n# Define any transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Create the dataset\ndataset = CustomImageDataset(\n    csv_file='data/labels.csv',\n    root_dir='data/images',\n    transform=transform\n)\n\n# Create the DataLoader\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n\n# Iterate through the DataLoader\nfor images, labels in dataloader:\n    print(images.shape, labels)\n</code></pre> <p>The class in this case implements <code>___len__()</code> and <code>__getitem__(idx)</code>. It allows, for example to a DataLoader, to direct access the entries, and so to shuffle the indices before accessing. Note that while the whole CSV file is loaded there in the constructor (for the labels and paths), only when an image is requested it is loaded. This way we can work with a large dataset of images, without running out of memory. The data-loader has support for multiple workers, so calls to <code>__getitem__</code> can be called in parallel to speed up things. A data-loader can potentially also make calls to <code>__getitem__</code> before we ask it for the next mini-batch (to pre-fetch). Looping over the mini-batches is done in the CPU's memory, and we can then move the tensors to the actual device (be it \"cuda\" for example.). It is usually expected that datasets return a tensor (or a tuple of tensors) from a call to <code>__get__</code> (those tensors are usually in the CPU's memory, till copied implicitly by our code that follows).</p> <p>When you want to use the same dataset class for training (when you have also the labels), and for inference (when you only have the features), a common approach is to return from <code>__getitem__</code> <code>(x, target)</code> when <code>self.target_exists</code> for example, while returning only <code>x</code> otherwise. During the initialization you can decide of the value of <code>self.target_exists</code> for example.</p>"},{"location":"training/#optimizers","title":"Optimizers","text":"<p>An optimizer manages the updates for the parameters.</p> <pre><code>import torch.optim as optim\n\n..\n\n\nmodel = YourModel()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# Training loop\nfor input, target in dataset:\n    optimizer.zero_grad()   # Clear gradients\n    output = model(input)   # Forward pass\n    loss = loss_fn(output, target)   # Compute loss\n    loss.backward()   # Backward pass\n    optimizer.step()   # Update parameters\n</code></pre>"},{"location":"training/#putting-it-together-a-training-loop","title":"Putting it together - a training loop","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\n\n# Define a simple neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load dataset and create DataLoader\ntransform = transforms.Compose([transforms.ToTensor()])\ndataset = datasets.MNIST(\n    root='./data',\n    train=True,\n    transform=transform,\n    download=True\n)\ntrain_set, val_set = random_split(dataset, [50000, 10000])\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n\n# Initialize model, loss function, and optimizer\nmodel = SimpleNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop with validation\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f'Epoch {epoch+1}/{num_epochs}, '\n          f'Training Loss: {running_loss/len(train_loader):.4f}, '\n          f'Validation Loss: {val_loss/len(val_loader):.4f}, '\n          f'Validation Accuracy: {100 * correct / total:.2f}%')\n</code></pre> <pre><code>...\n\nEpoch 1/5, Training Loss: 0.3167, Validation Loss: 0.1802, Validation Accuracy: 95.05%\nEpoch 2/5, Training Loss: 0.1429, Validation Loss: 0.1340, Validation Accuracy: 96.14%\nEpoch 3/5, Training Loss: 0.0973, Validation Loss: 0.1085, Validation Accuracy: 96.76%\nEpoch 4/5, Training Loss: 0.0735, Validation Loss: 0.0969, Validation Accuracy: 96.91%\nEpoch 5/5, Training Loss: 0.0562, Validation Loss: 0.0956, Validation Accuracy: 97.20%\n</code></pre> <p>Above should make sense by now. A couple of observations. On line 40 we see <code>model.train()</code> and on line 51 we see <code>model.eval()</code>. For some models, this has no effect and the models will work perfectly fine if we don't include those calls. There are however some modules (layers), that behave differently during training and evaluation (should behave so). So to be on the safe side, it is a good idea to call <code>model.train()</code> just before a training loop, and to call <code>model.eval()</code> when doing an evaluation. Why do we need evaluation interleaved in the training loop? Training a neural network is a long process (time-wise and money-wise when we are paying for the hardware by the hour). We should better see that something is moving in the right direction, or else that some weird things happen, as soon as possible. A second thing to notice, is that for the evaluation, we can save compute resources by working inside <code>with torch.no_grad()</code> context manager.</p> <p>Below is an example that includes also the reference to the device (in case we have a GPU available).</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Define a simple neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\n# Load dataset\ntransform = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n)\ntrain_dataset = datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform\n)\ntrain_loader = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=64,\n    shuffle=True\n)\n\n# Initialize the model, loss function, and optimizer\nmodel = SimpleNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        # Forward pass\n        outputs = model(data)\n        loss = criterion(outputs, target)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (batch_idx + 1) % 100 == 0:\n            print(f'''Epoch [{epoch + 1}/{num_epochs}],\nStep [{batch_idx + 1}/{len(train_loader)}],\nLoss: {loss.item():.4f}''')\n\nprint(\"Training complete!\")\n</code></pre>"}]}