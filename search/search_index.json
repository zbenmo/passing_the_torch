{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Passing the Torch","text":"<p>Where I teach myself PyTorch, and explore new capabilities. You are most welcome to learn along, or also to suggest corrections/additions.</p>"},{"location":"autograd/","title":"Autograd support with PyTorch tensors","text":"<p>The training for neural network often involves gradient descent. In gradient decent, which is an optimization method to find as good as possible values for parameters, such that the function calculated by the network shall give \"best\" objective function. The idea is that we look at the current output of the network and compare it to the desired output (to the target). If we call the difference (or some related function) the error. Then we want to minimize the error. We try to reduce the error a bit at any iteration. This process involves calculating gradients (partial derivaties). For example, let's say this is our function:</p> <p>\\hat{y} = a * x + 5</p> <p>Where a is the parameter, and at the start a = 1. We get an example of x = 1, and y = 7. It is not there yet, as for us we get 1 * 1 + 5 = 6. We note that \\frac{\\partial a}{\\partial y} = 1 In other words, a tiny increase of a say by \\epsilon a, shall result in a matching increase in the output. Let's increase a to 1.01. Now we get 1.01 * 1 + 5 = 6.01 which is indeed closer to the target. We actually work usually on the error, for example a * x + 5 - 7, calculate what will make it higher and go the other direction. The idea is the same. The derivatives are often more involved from above, as of multiple neurons, non-linearity, and as of composite functions. When a neuaral network is composed of multiple layers, this is basicly a composition of functions; L2(L1(Input)) and so. Calculating derivatives for composite functions is done usually with the help of the chain-rule, yet still involves a lot of calculations and is confusing.</p> <p>With neural networks of many parameters, we first calculate the gradients with respect to the error (or loss), for all the parameters, then change accordingly at the same time, a bit for each of the relevant parameters proportional to its gradient wrt the loss. As said above, coming up with the right formula of the partial derivates for all the parameters can be confusing and error prone. Luckily for us, with PyTorch we have a built-in support in the tensors, so that when you calculate a new tensor based on existing tensors, the existing tensors keep track of the gradients that are relevant to the calculation.</p> <pre><code># Create a tensor and indicate that we want to compute gradients\nx = torch.tensor(1.0, requires_grad=True)\n\n# Define a function of x\ny = x ** 2\n\n# Compute the gradient of y with respect to x\ny.backward()\n\n# The gradient is stored in x.grad\nprint(x.grad)  # Output: tensor(2.)\n</code></pre> <p>If we continue the example from above:</p> <pre><code>y2 = x ** 3\ny2.backward()\nprint(x.grad)\n</code></pre> <p><code>tensor(5.)</code></p> <p>It seems that the gradients for x were accumulated. Therefore we got both the gradients due to y as well as those due to y2.</p> <p>To reset and start from fresh:</p> <pre><code>y2 = x ** 3\nx.grad = None\ny2.backward()\nprint(x.grad)\n</code></pre> <p><code>tensor(3.)</code></p> <p>The calculations can be more involved, and as hinted above, involve multiple tensors and composed as in:</p> <pre><code>p1 = torch.tensor(1.0, requires_grad=True)\np2_1 = torch.tensor(2.0, requires_grad=True)\np2_2 = torch.tensor(3.0, requires_grad=True)\np3 = torch.tensor(1.5, requires_grad=True)\n\nx = 3\no1 = (p1 * x) ** 2 + 1\no2_1 = (p2_1 * o1) ** 0.5 - 4 \no2_2 = (p2_2 * o1).sin()\no3 = (p3 * (o2_1 + 2 * o2_2) + 9) ** 0.3\ny = 8 * o3\ny.backward()\n\nprint(p1.grad)\nprint(p2_1.grad)\nprint(p2_2.grad)\nprint(p3.grad)\n</code></pre> <pre><code>tensor(19.5745)\ntensor(1.0581)\ntensor(2.9195)\ntensor(-0.9488)\n</code></pre> <p>It seems to be right? No? You are welcome to verify. <code>:)</code></p> <p>With the following code (after pip installing torchviz):</p> <pre><code>from torchviz import make_dot\n\n# Visualize the computation graph\nmake_dot(y).render(\"computation_graph\", format=\"png\")\n</code></pre> <p>I've got the following picture:</p> Computation graph for y <p>For PyTorch models, one can use tensorboard to visualize the computation graphs (yes tensorboard supports also PyTorch). I was not able to achieve above on a \"stand alone\" tensor. It complained that the tensor does not have the attribute training, which is correct, yet probably not the intended outcome.</p> <p>While we haven't talked yet about neural networks, neighter about optimizers, and also not about data loaders, I'm bringing here a typical training loop, just to put in context autograd and backward.</p> <pre><code>...\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, data in enumerate(train_loader):\n        # Get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        # here the optimizer shall change the relevant model's parameters\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:  # Print every 100 mini-batches\n            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n            running_loss = 0.0\n\nprint(\"Finished Training\")\n</code></pre> <p>One needs to be a little aware when a calculation is part of a graph, and will result in the addition calculation of the gradients. There are a few reasons. Sometimes we're not training the model, but just making a use of its prediction capabilities (calculating gradients will waste resources). In other times we need to be aware that our calculation is outside the control of PyTorch autograd, for example, if we move stuff around for NumPy or using some external functionality / API. There are situations where we deliberty want some of the calculations to use autograd, while other not to. I bring here some related constructs, without giving the full context, yet we'll probably meet those constructs again later.</p> <pre><code># Disable autograd\nwith torch.no_grad(): # context manager\n    output = model(data)\n</code></pre> <pre><code>torch.autograd.set_grad_enabled(False)\n# Your code here\ntorch.autograd.set_grad_enabled(True)\n</code></pre> <pre><code>import torch\n\n# Create a tensor with gradient tracking enabled\nx = torch.randn(3, requires_grad=True)\n\n# Perform some operations\ny = x ** 2\n\n# Detach the tensor from the computation graph and convert to NumPy\nnumpy_array = y.detach().numpy()\n\nprint(numpy_array) # or maybe use matplotlib etc.\n# Not all plot's calculations should go all the way to to 'x' etc.\n</code></pre> <p>We can also disable the <code>requires_grad</code> for all, or some of, the relevant parameters of a model (the model's tensors). (will be shown elsewhere).</p>"},{"location":"neural_networks/","title":"Neural Networks with PyTorch","text":"<p>We're already seen tensors. Imagine a calculation, where at one side you have inputs, or readings from a sensor etc. At the other side of the calculation (the results) is a prediction, or a decision, or also some other desired output (such as a text describing in plain English what we see in the input image). The claim of neural networks (NN), is that such setting can be realized with a tensor or often multiple tensors, operating on the input(s), with potentially more mathematical operations on the way, and the output shall indeed be what we need it to be.</p> <p>Compare this way of thinking about a challenge to be automated, to traditional rule-based systems. Just a long, complicated and composed as needed, but with NN, just a feed-forward calculation. Compared above to a series of (nested) if-then-else and relevant logic. To be really honest, the calculations of a NN involve functions that \"contain\" if-like logic, such as .abs, .clip, and more. Also the rule-based solutions start with processing the inputs and extracting statistics and \"features\", such as: \"number of words on avarage in each sentence\", or \"total sum of purchases that month\", etc.</p> <p>The magic for a NN happens when the tensors contain the right values in each of their entries. All those values in the tensors, work together, as to \"extract statistics and features\" from the input(s), pass those throught the \"if-like\" logic, and output the desired result. Unlike traditional programming, filling the right values in the tensors, is done by a training algorithm (optimization) rather than by setting those following a clever analytics.</p> <p>While we could work with bare tensors, PyTorch has higher level construct nn.Module.</p> <pre><code>import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(5, 10)\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.tanh(x)\n        x = self.fc2(x)\n        return F.tanh(x)\n\n\nmodel = MyModel()\n\nout = model(torch.rand(5))\nprint(out)\nprint(out.shape)\n</code></pre> <pre><code>tensor([0.0552], grad_fn=&lt;TanhBackward0&gt;)\ntorch.Size([1])\n</code></pre> <p>Above, we've derived a new class from nn.Module. Out NN model is a nn.Module. We'll have a couple of attributes to each instance, that is in the example fc1 and fc2. In the constructor, those are just created and saved for later. nn.Linear is basicly two tensors. In the example of fc1, one of size [5, 10] and the other of size [5, 1] (bias). The operation implied by nn.Linear is basically matmul bewteen the input and the first tensor we then add the bias tensor. fc stands for \"fully connected\", this name is used often for nn.Linear. The internal modules are the \"layers\" of the neural network.</p> <pre><code>model.fc1.state_dict()\n</code></pre> <pre><code>OrderedDict([('weight', tensor([[-0.1497, -0.2375,  0.1056, -0.3654,  0.3629],\n        [ 0.0529,  0.3939, -0.0946,  0.3917,  0.3135],\n        [-0.3274, -0.0793,  0.1388,  0.2727, -0.3741],\n        [ 0.2795, -0.2330, -0.0990,  0.4247,  0.4342],\n        [ 0.1221,  0.2291, -0.1657,  0.4082,  0.2261],\n        [ 0.2819,  0.4004, -0.4413,  0.0400,  0.4329],\n        [ 0.2175,  0.0362,  0.2990,  0.3109, -0.2700],\n        [ 0.2692, -0.3269, -0.3591, -0.0944, -0.2211],\n        [ 0.2267, -0.4393,  0.3136,  0.2568,  0.0721],\n        [-0.3871, -0.3472, -0.1821, -0.4447,  0.2364]])), ('bias', tensor([-0.1682, -0.3213,  0.2310, -0.2638,  0.4306,  0.0138, -0.0102, -0.2979,\n        -0.2500, -0.3362]))])\n</code></pre> <p>The values of the tensors where randomly initialized.</p> <p>nn.Linear is by itself an nn.Module:</p> <pre><code>isinstance(model.fc1, nn.Module)\n</code></pre> <p><code>True</code></p> <p>It is nice that we compose nn.Module of other modules, as we can define custom \"building blocks\" and use them later as part of a NN model (imagine you want to repeat a construct a few times). It makes everything more \"modular\".</p> <p>In forward we make use of the internal layers on the module. We pass the input throught them and let them do their calculations. In the example above we broke the linearity between fc1 and fc2 using tanh. tanh does not have parameters, and so we did not need to allocate attributes in the constructor, but rather using it from torch.nn.functional, which is a collection of functions.</p> <p>Note that when we use the model above <code>out = model(torch.rand(5))</code>, we've \"called\" the model which in turns calls forward. We passed a single input, this was the expected input, a vector of 5 elements. We can also pass a mini-batch of inputs <code>out = model(torch.rand(5).unsqueeze(0))</code>. The output in the first case is <code>torch.Size([1])</code> and in the mini-batch one is <code>torch.Size([1, 1])</code>. I think we've have single element / mini-batch for free as of the broadcast capabilities of tensors. One can also have a mini-batch of mini-batches (if makes sense) etc.</p> <p>Note that the tensors where defined as ones that require autograd. This means that when we use the result of the network, potentially calculating the loss from the result and some desired output, we can backward the gradient to all relevant calculations that were used for the loss and internally in the forward implementation.</p> <p>With tensorboard, I got the following diagram:</p> <pre><code># make sure first to \"pip install tensorboard\"\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\nwriter.add_graph(model, torch.rand(5))\nwriter.close()\n# now open tensorboard --logdirs=runs and check for the graph of the latest run\n</code></pre> Computation graph for model <p>One can add the non-linearity also to the graph with a bit more attributes to the model:</p> <pre><code>class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(5, 10)\n        self.nl1 = nn.Tanh()\n        self.fc2 = nn.Linear(10, 1)\n        self.nl2 = nn.Tanh()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.nl1(x)\n        x = self.fc2(x)\n        return self.nl2(x)\n\n\nmodel = MyModel()\n</code></pre> <p>nn.Module brings some more goodies. For example, while we've added attributes to our class, those \"parameters\" where also noted, and are currently available (see above usage of <code>state_dict</code>).</p> <pre><code>list(model.parameters())\n</code></pre> <pre><code>[Parameter containing:\n tensor([[ 0.2207,  0.1466, -0.3568, -0.2860, -0.0585],\n         [-0.1582,  0.1217, -0.0859,  0.1306,  0.1662],\n         [-0.0158,  0.4182, -0.3181,  0.0978, -0.1387],\n         [ 0.1375,  0.3619,  0.2441, -0.3241, -0.0193],\n         [-0.1960,  0.4269, -0.2684,  0.0177,  0.4463],\n         [-0.1160, -0.0506, -0.2181,  0.1954,  0.2758],\n         [-0.3748,  0.1646, -0.1075, -0.1940, -0.4468],\n         [ 0.3785,  0.1593, -0.3964, -0.0366, -0.2333],\n         [ 0.1707, -0.0325, -0.0823,  0.0911, -0.3582],\n         [ 0.1998, -0.1256,  0.2642, -0.2502,  0.2250]], requires_grad=True),\n Parameter containing:\n tensor([ 0.0451,  0.3004, -0.0041,  0.4164, -0.0115, -0.3451, -0.3801, -0.1956,\n         -0.1690,  0.3703], requires_grad=True),\n Parameter containing:\n tensor([[ 0.1877, -0.3158,  0.0916,  0.1542,  0.1127,  0.0930,  0.1966,  0.2425,\n           0.0706,  0.2424]], requires_grad=True),\n Parameter containing:\n tensor([0.2331], requires_grad=True)]\n</code></pre> <p>Values above may differ as I ran the snippet in another opportunity. The ability to have the <code>state_dict</code> of a model will help us when we want to save a copy of a model on disk. If we fail to save the parameters, we'll need to train and verify each time we need such a model. The real value of a model is one that was trained and gave us good results on unseen dataset. <code>parameters</code> will be useful for us when we want to select what parameters to train / fine-tune.</p> <p>Please note that a <code>nn.Module</code> is aware only of other <code>`nn.Module</code> that is an attribute of the object. Not sure how this wonderful capability was achieved in the first place, yet we do need to be aware of it. Let's try the following:</p> <pre><code>class MyModel(nn.Module):\n    def __init__(self, num_layers=3):\n        super(MyModel, self).__init__()\n        self.layers = []\n        in_features = 5\n        out_features = 10\n        for _ in range(num_layers - 1):\n            self.layers.append(nn.Linear(in_features, out_features))\n            in_features = out_features   \n        self.layers.append(nn.Linear(in_features, 1))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n            x = F.tanh(x)\n        return x\n\n\nmodel = MyModel()\nmodel.state_dict()\n</code></pre> <p><code>OrderedDict()</code></p> <p>The funny part, is that above mostly do work:</p> <pre><code>model(torch.rand(5))\n</code></pre> <p><code>tensor([-0.3304], grad_fn=&lt;TanhBackward0&gt;)</code></p> <p>Easy solution for above is to use <code>nn.ModuleList</code></p> <pre><code>class MyModel(nn.Module):\n    def __init__(self, num_layers=3):\n        super(MyModel, self).__init__()\n        layers = []\n        in_features = 5\n        out_features = 10\n        for _ in range(num_layers - 1):\n            layers.append(nn.Linear(in_features, out_features))\n            in_features = out_features   \n        layers.append(nn.Linear(in_features, 1))\n        self.layers = nn.ModuleList(layers)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n            x = F.tanh(x)\n        return x\n\n\nmodel = MyModel(2)\nmodel.state_dict()\n</code></pre> <pre><code>OrderedDict([('layers.0.weight',\n              tensor([[ 0.3956, -0.3880,  0.4201, -0.1354, -0.1536],\n                      [ 0.1374,  0.0503,  0.0612,  0.0231,  0.1588],\n                      [-0.0798, -0.2296, -0.0735,  0.3649,  0.3362],\n                      [ 0.2273,  0.3117,  0.1105, -0.0899,  0.3984],\n                      [-0.2068, -0.1243, -0.0746, -0.0251,  0.0485],\n                      [-0.1613, -0.2822, -0.2108, -0.1591,  0.0344],\n                      [ 0.0259, -0.3709, -0.3758, -0.1356,  0.3477],\n                      [-0.3030,  0.0275, -0.3157,  0.1007,  0.2047],\n                      [-0.0222,  0.0705,  0.2005, -0.3808, -0.0279],\n                      [ 0.0557,  0.2277,  0.3824, -0.3172,  0.1456]])),\n             ('layers.0.bias',\n              tensor([ 0.2578, -0.0806,  0.1190,  0.2998,  0.2751,  0.1743,  0.2498, -0.1855,\n                      -0.3150,  0.1348])),\n             ('layers.1.weight',\n              tensor([[ 0.0866, -0.0983, -0.1061, -0.1938,  0.2220, -0.0435, -0.1673,  0.2343,\n                       -0.1903, -0.2391]])),\n             ('layers.1.bias', tensor([0.0450]))])\n</code></pre> <p>One can also use <code>nn.Sequential</code> which includes also the cascading loop.</p> <pre><code>class MyModel(nn.Module):\n    def __init__(self, num_layers=3):\n        super(MyModel, self).__init__()\n        self.layers = nn.Sequential()\n        in_features = 5\n        out_features = 10\n        for _ in range(num_layers - 1):\n            self.layers.extend([nn.Linear(in_features, out_features), nn.Tanh()])\n            in_features = out_features   \n        self.layers.extend([nn.Linear(in_features, 1), nn.Tanh()])\n\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nmodel = MyModel()\n</code></pre> Sequential (as shown on tensorboard if you add the graph of the model)"},{"location":"pytorch_vs_numpy/","title":"PyTorch as a Linear-Algebra support package","text":"<p>PyTorch provides utilities to manipulate numbers with linear-algebra style logic and operations. A data point with two values, for example age and height, can be represented by a vector with two items, where the first entry is the age, and the second entry is the height.</p> <pre><code>import torch\n\n\n# first entry age in years, second entry height in centimeters\nperson_details = torch.tensor([18, 172.4])\n\nperson_details\n</code></pre> <p><code>tensor([ 18.0000, 172.4000])</code></p> <p>Why not a dataclass (or just a Python class)? Why not a Python dict? It is still a good practice to use more structured data types in your program and databases. At one stage to communicate the data to machine learning; linear models, neural networks, etc., we shall need probably to work with this kind of representation, meaning vectors, matrices, and higher dimensions tensors, with all entries of the same (numeric) type.</p> <p>If you are familiar with DataFrames from Pandas, Polars, or Spark, or if you have already used tabular data with ML libraries such as Scikit-Learn, this seems to be a step backwards. Yet think of streams of images from cameras (a video or so), embedding for variable-length texts, sound, the feed from sensors, etc. For more flexibility with the inputs and the outputs, getting used to tensor representation will help us with addressing those challenges. We'll need to find the right combination of \"human facing\" abstractions and \"neural network facing\" representation, and where to do the back and forward translations.    </p> <p>So a vector is tensor with one dimention. To continue with above example, we can have the details for more than one person, in a matrix. Each row will represent another person (another data point.).</p> <pre><code>..\n\n# first entry age in years, second entry height in centimeters\nsubjects_details = torch.tensor([\n    [18, 172.4],\n    [51, 169.2],\n    [46, 164.5],\n])\n\nsubjects_details\n</code></pre> <pre><code>tensor([[ 18.0000, 172.4000],\n        [ 51.0000, 169.2000],\n        [ 46.0000, 164.5000]])\n</code></pre> <p>If you are saying at this stage, \"hey, this is just like NumPy\", then you are right. Tensors in PyTorch are like arrays in NumPy. The functionality of NumPy is mostly available also with PyTorch, and even the inteface of PyTorch was inspired by NumPy and is very similar. So why yet another implementation?</p> <p>PyTorch's tensors bring additional functionalities that are not found in NumPy. Trying to add the functionalities, we'll describe those later, to NumPy, would have slowed the process and Meta, the developers behind PyTorch wanted to run fast. Also NumPy have some target audience and relevant optimizations, that are different from what the creators of PyTorch needed. PyTorch is optimized towards the usage with neural networks. NumPy is more memory optimized.</p> <p>There are still many ideas in NumPy worth getting to know, that are also relevant while using PyTorch. For example the concept of vectorization, which is, instead of looping over the entries and applying an operation to each entry at a time, issue the operation on the whole set of entries at once, where possible, letting the implementation by the package use hardware and other tricks to apply \"at the same time\" to all entries, or to batches of inputs, transparently to the caller (ex. instead of applying 1,000 times applying 10 times for each batch/slice of size 100). </p> <p>An example of things you can do with PyTorch (and with Numpy) is the following: let's say we want to calculate the BMI ( Body Mass Index) for people, based on the following formula:</p>  BMI = {mass(kg) \\over height^2(m)}  <pre><code>..\n\n# first entry age in years, second entry height in centimeters,\n# and just added the mass in kilograms.\nsubjects_details = torch.tensor([\n    [18, 172.4, 62.1],\n    [51, 169.2, 66.0],\n    [46, 164.5, 72.0],\n])\n\nsubjects_BMI = (\n    subjects_details[:, 2] / (subjects_details[:, 1] / 100.0) ** 2\n)\n\nsubjects_BMI\n</code></pre> <p><code>tensor([20.8938, 23.0538, 26.6073])</code></p> <p>What we did there? We picked the third column (index 2) and devided it (element wise) with the second column (index 1), after the last was first converted to meters, and then raised to the power of 2. If I read the results correct, then the third person is a bit overweight.</p> <p>A side note. Is NumPy obsolete? Can we forget about NumPy, or never bother to learn it if did not play with it yet? My answer will be that at the moment it is still around and is well supported and integrated by and with many packages. Also I believe it is a dependancy of PyTorch (probably not making use of NumPy, yet as of back-and-forward conversions.). It is very easy to initialize a PyTorch tensor from a NumPy array and vice versa. So keep learning PyTorch, and use NumPy if and where needed.</p> <p>It is a good practice to verify the type of a Python object, when in doubt.</p> <pre><code>type(subjects_details)\n</code></pre> <p><code>torch.Tensor</code></p> <p>To get a bit more information with tensors, use also the following:</p> <pre><code>subjects_details.type()\n</code></pre> <p><code>'torch.FloatTensor'</code></p> <pre><code>subjects_details.dtype\n</code></pre> <p><code>torch.float32</code> (the elements are all of torch.float32 type)</p> <pre><code>subjects_details.shape\n</code></pre> <p><code>torch.Size([3, 3])</code> (3 rows and 3 columns right after we've added also the mass)</p> <p>Tensors are very useful also to represent images.</p> <pre><code>..\nimg = torch.tensor([\n    [0, 1, 1, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 1],\n    [0, 1, 1, 0],\n])\n\nimg\n</code></pre> <pre><code>tensor([[0, 1, 1, 0],\n        [1, 0, 0, 1],\n        [1, 0, 0, 1],\n        [0, 1, 1, 0]])\n</code></pre> <pre><code>img.type()\n</code></pre> <p><code>'torch.LongTensor'</code></p> <p>Above the implementation of <code>torch.tensor</code> noted that all the entries are integers and so we got a <code>LongTensor</code>. If you want to have floats in the first place, use <code>tensor.Tensor</code> instead:</p> <pre><code>img = torch.Tensor([\n    [0, 1, 1, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 1],\n    [0, 1, 1, 0],\n])\n\nimg\n</code></pre> <pre><code>tensor([[0., 1., 1., 0.],\n        [1., 0., 0., 1.],\n        [1., 0., 0., 1.],\n        [0., 1., 1., 0.]])\n</code></pre> <p>Alternatively you can convert an existing tensor to another type, by a match operation, or explicitly using <code>to</code>:</p> <pre><code>img = torch.tensor([\n    [0, 1, 1, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 1],\n    [0, 1, 1, 0],\n])\n\nimg.to(torch.float32)\n</code></pre> <pre><code>tensor([[0., 1., 1., 0.],\n        [1., 0., 0., 1.],\n        [1., 0., 0., 1.],\n        [0., 1., 1., 0.]])\n</code></pre> <p>Images often have multiple channels (ex. RGB). With PyTorch the channel should often come before the height and the width, so if you happen to have HWC and you want to \"change\" it into CHW, you can do the following:</p> <pre><code>img = ... # a tensor representing an image, given in HWC format.\nimg = img.permute([2, 0, 1]) # let's have it in CHW format.\n</code></pre> <p>To verify above snippet of code, I've ended up creating first CHW image and then converting it to HWC (just to convert it back):</p> <pre><code>chw_img = img.unsqueeze(0).repeat(3, 1, 1)\nchw_img\n</code></pre> <p>unsqeeuze adds a dimention. repeat in this example \"inflates\" it from B/W to RGB (three channels).</p> <pre><code>tensor([[[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]]])\n</code></pre> <pre><code>chw_img[1] *= 2\nchw_img[2] *= 3\nchw_img\n</code></pre> <pre><code>tensor([[[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 2, 2, 0],\n         [2, 0, 0, 2],\n         [2, 0, 0, 2],\n         [0, 2, 2, 0]],\n\n        [[0, 3, 3, 0],\n         [3, 0, 0, 3],\n         [3, 0, 0, 3],\n         [0, 3, 3, 0]]])\n</code></pre> <p>Lastly let's show what we wanted, so if the channel is the last dimension HWC:</p> <pre><code>hwc_img = chw_img.permute(1, 2, 0)\nhwc_img[:, :, 2]\n</code></pre> <pre><code>tensor([[0, 3, 3, 0],\n        [3, 0, 0, 3],\n        [3, 0, 0, 3],\n        [0, 3, 3, 0]])\n</code></pre> <p>Above demonstrates what the third channel (Blue?) looks like. Now we can change the channel to be in the first dimension (as usually is assumed with PyTorch).</p> <pre><code>hwc_img.permute(2, 0, 1)\n</code></pre> <pre><code>tensor([[[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 2, 2, 0],\n         [2, 0, 0, 2],\n         [2, 0, 0, 2],\n         [0, 2, 2, 0]],\n\n        [[0, 3, 3, 0],\n         [3, 0, 0, 3],\n         [3, 0, 0, 3],\n         [0, 3, 3, 0]]])\n</code></pre> <p>It is usually a good practice to operate on tensors and get new tensors as a result. This is more \"functional\" in the sense that code and functions that we run do not have side effects on the inputs, and shall return the same results (subject to implicit randomness), if called again with the same inputs. If we do need to save space, we can opt in to \"in place\" operations.</p> <p>A very useful functionality of tensor is reshape. Imagine a 2d image. Its pixels' values may be stored on disk in a sequential mannner. When we load the image, we are starting with a 1d-tensor. Then maybe we want to process the image with its \"georgaphy\" in mind (cropping for example).</p> <pre><code>img_vec = torch.arange(9)\nprint(img_vec.shape)\nimg_mat = img_vec.reshape(-1, 3)\nprint(img_mat.shape)\n</code></pre> <pre><code>torch.Size([9])\ntorch.Size([3, 3])\n</code></pre> <p>Above we've suggested that there are 3 columns, and let PyTorch do the math regarding the rows (this was the -1).</p> <pre><code>img_mat\n</code></pre> <pre><code>tensor([[0, 1, 2],\n        [3, 4, 5],\n        [6, 7, 8]])\n</code></pre> <p>When makes sense, PyTorch attempts to avoid moving memory from place to place, but rather only keep \"management\" information regarding the shape. This however means that there is only one \"place\":</p> <pre><code>img_mat[-1, -1] = 88\nimg_vec\n</code></pre> <pre><code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7, 88])\n</code></pre> <p>You may see in places the usage of .view() which is like .reshape() yet may fail when the memory is not continous. So if you want to make sure the memory is continous, use .view(), if you want it just to work, use .reshape().</p> <p>We've seen above element wise operations, for example when we've computed BMI. PyTorch, as does NumPy, supports also linear algebra style vector and higher-order tensors operations, such as multiplication.</p> <pre><code>tensor1 = torch.randn(3)\ntensor2 = torch.randn(3)\nres = torch.matmul(tensor1, tensor2)\nprint(res)\nprint(res.size())\nprint(res.item())\n</code></pre> <pre><code>tensor(-1.0853)\ntorch.Size([])\n-1.0853235721588135\n</code></pre> <p>We got above a 0-dimensions tensor (a scalar). In order to take the single value out of the tensor, we've used .item().</p> <p>Note that the mathematical rules dictate the the dimensions in above example should have been [1, 3] and [3, 1], yet .matmul figures out what we want to do there.</p>"},{"location":"pytorch_vs_numpy/#broadcasting","title":"Broadcasting","text":"<p>When we need element-wise operations, such as addition, the tensors are supposed to be of the same size, that is same dimensios, and same number of elements at each dimension. But we've already seen a division by scalar, which means that every element is diveded by the same scalar, or you can also think of it that a conceptual tensor is created, and the scalar is broadcasts to all elements of the new tensor, and then we have the element-wise division. Broadcasting is even more general. If you have two tensors, with different sizes, it can still work, if it is possible to \"understand\" what is meant; starting from the last dimension, if it is equal among the tensors, we move to the next one. If one of the dimentions is 1, then we assume all entries of the conceptual tensor are the same. If we ran out of dimension in one tensor, we can assume we have there 1 as above.</p> <p>Here are some examples that should work:</p> <pre><code>[3, 4, 5] and [3, 1, 5]\n[1, 2, 2] and [2, 2, 2]\n[2, 2] and [2, 2, 2]\n</code></pre> <p>And here are some examples that should probably don't work:</p> <pre><code>[3, 3] and [3, 6]\n[2, 2] and [3, 2]\n</code></pre> <p>If we did not have the support of broadcasting, we could have just do as above with unsqueeze, squeeze, repeat, etc. However it makes our life a bit easier, and potentially saves memory and computations.</p> <pre><code>img = torch.tensor([\n    [0, 1, 1, 0],\n    [1, 0, 0, 1],\n    [1, 0, 0, 1],\n    [0, 1, 1, 0],\n])\n\nimg * torch.tensor([[[1]], [[2]], [[3]]]) # [4, 4] and [3, 1, 1]\n</code></pre> <pre><code>tensor([[[0, 1, 1, 0],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [0, 1, 1, 0]],\n\n        [[0, 2, 2, 0],\n         [2, 0, 0, 2],\n         [2, 0, 0, 2],\n         [0, 2, 2, 0]],\n\n        [[0, 3, 3, 0],\n         [3, 0, 0, 3],\n         [3, 0, 0, 3],\n         [0, 3, 3, 0]]])\n</code></pre>"},{"location":"pytorch_vs_numpy/#support-for-gpu","title":"Support for GPU","text":"<p>Tensors can be allocated on the GPU's memory, if you have such. Relevant operations on the tensors will run there (on the GPU), often faster. Operations between tensors need to be done when both tensors are located on the same device. If you detect that you have a GPU, and you train a model, putting the parameters of the model on the GPU, the training and evaluation data, should also be at the time on the GPU. Data is often loaded from the disk into the main memory, and then mini-batches are copied to the GPU's memory, used for an iteration, and make space for the next mini-batches, that are waiting either in memory or still on the disk. NumPy as far as I know does not have support for GPU. There is another NumPy like package called Numba, that apparently does support GPU. I haven't played with Numba yet.</p> <pre><code>torch.cuda.is_available()\n</code></pre> <pre><code>True\n</code></pre> <p>Above was shown on Colab. If like me you don't have GPU in your private environment, consider using Colab.</p> <pre><code>tensor1 = torch.randn(3)\ntensor1.device\n</code></pre> <p><code>device(type='cpu')</code></p> <pre><code>tensor1_cuda = tensor1.to(\"cuda\")\ntensor1_cuda.device\n</code></pre> <p><code>device(type='cuda', index=0)</code></p> <p>Just to iterate, operations between tensors that are located on different devices, are not supposed to work:</p> <pre><code>...\n\ntensor1 = torch.randn(3)\ntensor2 = torch.randn(3).to(\"cuda\")\ntensor1 + tensor2\n</code></pre> <pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-2-5076b2d4339c&gt; in &lt;cell line: 3&gt;()\n      1 tensor1 = torch.randn(3)\n      2 tensor2 = torch.randn(3).to(\"cuda\")\n----&gt; 3 tensor1 + tensor2\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n</code></pre> <p>If you do want to add as an example two tensors that are on different device, you need first to move one of them to the same device where the other is. Remember that the GPU is fast, while the CPU (the relevant memory) is potentially bigger, but also when needed, backed by even larger space transparently (the operating system's virtual memory and the disk). </p> <p>There is more to PyTorch of course, and also related to tensors. We'll talk about auto grad next.</p>"},{"location":"training/","title":"Training","text":"<p>We already have a model, which knows to feed-forward some input(s) in a specific size (shape) and return specific size output(s). We've seen an easy and modular way to define those, in a way that keeps track of all our parameters, and that calculate for us partial derivatives, needed for gradient descent. We need now a training dataset. A dataset can be given for example as a tensor with the appropriate dimensions and size. The first dimension can be the instance (data item), so that we actually get a batch of our examples. For simplicity of the discussion, let's assume we're in supervised setting, that means that for every entry in the batch we have also a label.</p> <pre><code>import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ndef get_dataset():\n    X = torch.rand(20, 5)\n    y = torch.matmul(X, torch.tensor([0.3, 4.0, 0.2, 2.5, 18.0])) + 17.0\n    return X, y\n\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(5, 1)\n\n    def forward(self, x):\n        return self.fc1(x)\n\n\nX, y = get_dataset()\nprint(f'{X.shape=}', f'{y.shape=}')\nmodel = MyModel()\n\nlr = 1e-3 # learing rate\n\nfor epoch in range(10):\n    output = model(X)\n    assert output.shape == torch.Size((20, 1))\n    loss = F.mse_loss(output.squeeze(), y)\n    # the target (y) is the second parameter\n    # (yes in scikit-learn it is the other way around)\n    print(f'{loss=}')\n    for parameter in model.parameters():\n        parameter.grad = None\n    loss.backward() # send the grads to the model's parameters\n    for parameter in model.parameters():\n        parameter.data -= lr * parameter.grad\n</code></pre> <pre><code>X.shape=torch.Size([20, 5]) y.shape=torch.Size([20])\nloss=tensor(880.1790, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(872.0485, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(863.9950, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(856.0178, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(848.1162, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(840.2894, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(832.5369, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(824.8579, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(817.2516, grad_fn=&lt;MseLossBackward0&gt;)\nloss=tensor(809.7174, grad_fn=&lt;MseLossBackward0&gt;)\n</code></pre> <p>Note that we assumed that the whole dataset can fit into our device's memory, be it the \"cpu\" or \"cuda\" (for GPU). Often with gradient descent we use mini-batches and each epoch is actually a loop itself where on each iteration a single mini-batch is feed-forwarded and then the lesson regarding the loss is learned. The main reason for working with mini-batches is memory limitations, and there might some others. PyTorch has nice support for datasets and data loaders, that shall be shown shortly.</p> <p>The other element I want to bring up is that I've used above the grad of a parameter to update its data. On each epoch, I've updated wrt to the gradients, but took from it only <code>lr</code>. There are even better methods to optimize based on the gradients, potentially remembering what we have done in previous rounds, and to be \"consistent\" in the updates, or to keep the momentum. There are other smarter things to do, including to adapt the <code>lr</code> with time, or to have a different learning rate for different parameters. A lot of this logic is supported with optimizers. We'll see that also later. But first let's look datasets and data loaders.</p>"},{"location":"training/#datasets-and-data-loaders","title":"Datasets and Data Loaders","text":"<pre><code>import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\n\n# Define a transform to convert the data to tensor\ntransform = transforms.ToTensor()\n\n# Download and load the training data\ntrain_data = datasets.FashionMNIST(\n    root='data',\n    train=True,\n    download=True,\n    transform=transform\n)\n\n# Download and load the test data\ntest_data = datasets.FashionMNIST(\n    root='data',\n    train=False,\n    download=True,\n    transform=transform\n)\n\n# Create data loaders\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n\n# Visualize some training data\ndef show_images(images, labels):\n    plt.figure(figsize=(10, 10))\n    for i in range(25):\n        plt.subplot(5, 5, i+1)\n        plt.imshow(images[i].numpy().squeeze(), cmap='gray')\n        plt.title(labels[i].item())\n        plt.axis('off')\n    plt.show()\n\n# Get a batch of training data\nimages, labels = next(iter(train_loader))\nshow_images(images, labels)\n</code></pre> <pre><code>0.9%\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n100.0%\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n100.0%\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n\n14.8%\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n100.0%\n100.0%\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n</code></pre> Samples from fashion MNIST <p>Above example we have made a usage of a sample dataset found in the useful Python package <code>torchvision</code>, that is <code>FashionMNIST</code>. The nice thing about using a dataset is that it hides details such as where the files are (already loaded to memory, local disk, on the Internet, etc.). Also we had two such datasets. One for training and another for testing. And we had a chance to add transformations (which then applied be item). In this case we've made sure the items are given to us as PyTorch tensors. We did not access the datasets directly but rather used a utility class <code>DataLoader</code> that allows us to define the batch (mini-batch) size, as well as wheater we want to shuffle the entries first. Shuffeling may be beneficial for the training, but is not important for the test. Out of the 64 images that we get in the first mini-batch, we show above the first 5x5 images (and their supervised labels).</p> <p>One often defines there own dataset.</p> <pre><code>class CustomImageDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n        image = io.imread(img_path)\n        label = int(self.annotations.iloc[idx, 1])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n\nfrom torchvision import transforms\n\n# Define any transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Create the dataset\ndataset = CustomImageDataset(\n    csv_file='data/labels.csv',\n    root_dir='data/images',\n    transform=transform\n)\n\n# Create the DataLoader\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n\n# Iterate through the DataLoader\nfor images, labels in dataloader:\n    print(images.shape, labels)\n</code></pre> <p>The class in this case implements <code>___len__()</code> and <code>__getitem__(idx)</code>. It allows, for example to a DataLoader, to direct access the entries, and so to shuffle the indices before accessing. Note that while the whole CSV file is loaded there in the constructor (for the labels and paths), only when an image is requested it is loaded. This way we can work with a large dataset of images, without running out of memory. The data-loader has support for multiple workers, so calls to <code>__getitem__</code> can be called in parallel to speed up things. A data-loader can potentially also make calls to <code>__getitem__</code> before we ask it for the next mini-batch (to pre-fetch). Looping over the mini-batches is done in the CPU's memory, and we can then move the tensors to the actual device (be it \"cuda\" for example.). It is usually expected that datasets return a tensor (or a tuple of tensors) from a call to <code>__get__</code> (those tensors are usually in the CPU's memory, till copied implicitly by our code that follows).</p>"},{"location":"training/#optimizers","title":"Optimizers","text":"<p>An optimizer manages the updates for the parameters.</p> <pre><code>import torch.optim as optim\n\n..\n\n\nmodel = YourModel()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# Training loop\nfor input, target in dataset:\n    optimizer.zero_grad()   # Clear gradients\n    output = model(input)   # Forward pass\n    loss = loss_fn(output, target)   # Compute loss\n    loss.backward()   # Backward pass\n    optimizer.step()   # Update parameters\n</code></pre>"},{"location":"training/#putting-it-together-a-training-loop","title":"Putting it together - a training loop","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\n\n# Define a simple neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Load dataset and create DataLoader\ntransform = transforms.Compose([transforms.ToTensor()])\ndataset = datasets.MNIST(\n    root='./data',\n    train=True,\n    transform=transform,\n    download=True\n)\ntrain_set, val_set = random_split(dataset, [50000, 10000])\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n\n# Initialize model, loss function, and optimizer\nmodel = SimpleNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop with validation\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f'Epoch {epoch+1}/{num_epochs}, '\n          f'Training Loss: {running_loss/len(train_loader):.4f}, '\n          f'Validation Loss: {val_loss/len(val_loader):.4f}, '\n          f'Validation Accuracy: {100 * correct / total:.2f}%')\n</code></pre> <pre><code>...\n\nEpoch 1/5, Training Loss: 0.3167, Validation Loss: 0.1802, Validation Accuracy: 95.05%\nEpoch 2/5, Training Loss: 0.1429, Validation Loss: 0.1340, Validation Accuracy: 96.14%\nEpoch 3/5, Training Loss: 0.0973, Validation Loss: 0.1085, Validation Accuracy: 96.76%\nEpoch 4/5, Training Loss: 0.0735, Validation Loss: 0.0969, Validation Accuracy: 96.91%\nEpoch 5/5, Training Loss: 0.0562, Validation Loss: 0.0956, Validation Accuracy: 97.20%\n</code></pre> <p>Above should make sense by now. A couple of observations. On line 40 we see <code>model.train()</code> and on line 51 we see <code>model.eval()</code>. For some models, this has no effect and the models will work perfectly fine if we don't include those calls. There are however some modules (layers), that behave differently during training and evaluation (should behave so). So to be on the safe side, it is a good idea to call <code>model.train()</code> just before a training loop, and to call <code>model.eval()</code> when doing an evaluation. Why do we need evaluation interleaved in the training loop? Training a neural network is a long process (time-wise and money-wise when we are paying for the hardware by the hour). We should better see that something is moving in the right direction, or else that some weird things happen, as soon as possible. A second thing to notice, is that for the evaluation, we can save compute resources by working inside <code>with torch.no_grad()</code> context manager.</p>"}]}